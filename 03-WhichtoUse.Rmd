# Which should I use: a run chart or a control chart? {#which}

A run chart can monitor a metric or a process, while a control chart monitors a process. Always create a run chart. Create a control chart only if you meet the necessary conditions.  

In both cases, the data points must be independent, that is, the position of one point does not influence the position of another point: there is no (serious) autocorrelation. If the data are autocorrelated, the guidelines for testing run or control charts can be invalid, which can lead to poor decision-making.   

## Which to use

| Use a run chart if | Use a control chart (only) if |
| ------------------------------------ | ------------------------------------- |
| You may or may not investigate or act when a data point crosses a reference, target, or goal level, or when guidelines suggest a non-random pattern is occurring. | You intend to investigate or act when the process moves outside of control or indicates special cause variation. | 
|  |  | 
| You have little control over or cannot control the metric (e.g., ED volume/acuity). | You have the potential to control the process driving the metric (e.g., ED wait times). | 
|  |  | 
| You want to monitor the behavior of individual or groups of data points to a reference, target, or goal level. | You want to monitor the "average" of the system's behavior (i.e., the underlying statistical process)---deviations from expectations. | 
|  |  | 
| You are monitoring a metric or process that is generally trending or contains seasonality or other cycles of known cause, as long as you are able to adjust for any seasonality as well as able calculate an appropriate median line (e.g., via quantile regression for trending data). | You are monitoring a *stable* statistical process (there is no trend in the time series, or you have made the appropriate corrections to account or adjust for trends or seasonality). |
|  |  | 
| You have no expectations that normal day-to-day operations will affect the central tendency. | You expect that normal day-to-day operations will or are meant to keep the process stable within the bounds of common-cause variation. |
|  |  | 
| You do not need to account for the inherent natural variation in the system. | You need to understand and account for the inherent natural variation ("noise") in the system. | 
|  |  | 
| You have at least 12 data points. (Fewer than 12? Just make line chart, or use an EWMA chart. Run chart guidelines may not be valid.) | You have 20 or more data points that are in a stable statistical process, or you have performed a power analysis that provides the appropriate n for the appropriate time interval. | 
|  |  | 
| You do not understand one or more of the statistical issues discussed in the control chart column. | You understand the practical trade-offs between the sensitivity and specificity of the control limits relative to your need to investigate or act. |
|  |  | 
| | You know which statistical distribution to use to calculate the control limits to ensure you have the proper mean-variance relationship. |  


## Testing assumptions {#testassumptions}

### Trending

You can test whether a process is trending first by eye: does it look like it's trending over a large span of the time series? Then it probably is.  


```{r trendtest}
# Generate fake process data with an upward trend
set.seed(81)
n = 36
x = seq(1:n)
mb = data.frame(x = x, y = 10000 + (x * 1.25) + (rnorm(n, 0, 5)))

# Plot
ggplot(mb, aes(x, y)) + 
  xlab("Subgroup") + 
  ylab("Value") + 
  geom_line(color="gray70") +
  geom_point() 
```


More technically, you can run a simple linear regression---if the coefficient of the slope is significant, your data is trending.


```{r lm}
# Run linear model
mb_lm = lm(y ~ x, data=mb)
```

```{r lmdeets, echo=FALSE}
# Make a prettier table for the lm results
mb_table = broom::tidy(mb_lm)

fixed_digits <- function(xs, n = 2) {
  formatC(xs, digits = n, format = "f")
}

format_pval = function(ps, html = FALSE) {
  tiny = ifelse(html, "&lt;&nbsp;0.001", "< 0.001")
  ps_chr = ps %>% fixed_digits(3) 
  ps_chr[ps < 0.001] = tiny
  ps_chr
}

mb_table$p.value = format_pval(mb_table$p.value)

kable(mb_table, digits = 2, col.names = c("Parameter", "Coefficient", "SE", "t statistic", 
      "p-value"))
```

<br/>  

The Mann-Kendall trend test is often used as well:  

```{r mktest}
mb_ts = ts(mb$y, start = c(2010, 1), frequency = 12)
trend::mk.test(mb_ts)
```

Because trends can sometimes be an indication of special cause variation in a stable process, standard control limits don't make sense around long-trending data, and calculation of center lines and control limits will be incorrect. Thus, tests for special causes other than trending will also be invalid. Use a run chart with an appropriate median instead (e.g., via quantile regression), as seen in [Chapter 6](#runtrend).  

Non-linear change is a special case of trending data, and isn't necessarily autocorrelated, though it can be in cases like seasonality. When it *isn't* autocorrelated, advanced techniques like Generalized Additive Models (GAM) can be used in place of a control chart to provide limits that can assist decision-making, as seen in [Chapter 8](#GAM). If you were using a control chart when the data began to show trending, restart control limits after the trend has stabilized or switch to using a GAM.  

### Independence and autocorrelation

For either run or control charts, the data points must be independent for the guidelines to be effective. The first test of that is conceptual---do you expect that one value in this series will influence a subsequent value? For example, the incidence of some hospital-acquired infections can be the result of previous infections---say, one happens at the end of March, and another happens at the start of April in the same unit, and were caused by the same organism, you might suspect that the monthly values would not be independent. 

A second test is by calculating the autocorrelation function for the time series. Autocorrelation values over 0.50 generally indicate problems, as do patterns in the autocorrelation function, as seen in [Chapter 1](#timedep). However, *any* significant autocorrelation should be considered carefully relative to the cost of potential false positive or false negative signals.   

```{r acfexample, echo=FALSE}
p1 = autoplot(acf(beer, plot = FALSE)) 
ggsave("images/ac.png") 

p2 = autoplot(acf(df_ts, plot = FALSE))
ggsave("images/no_ac.png")
```

| Example autocorrelated data | Example non-autocorrelated data |
| ----------------------------------- | ------------------------------------- |
| ![](images/ac.png) | ![](images/no_ac.png) |

When data are autocorrelated, control limits will be *too small*---and thus an increase in *false* signals of special causes should be expected. In addition, none of the tests for special cause variation remain valid.    

Sometimes, autocorrelation can be removed by changing the sampling or metric's time step: for example, you generally wouldn't expect hospital acquired infection rates in one quarter to influence those in the subsequent quarter. It can also be sometimes removed or abated with differencing, although doing so hurts interpretability of the resulting run or control chart. 

```{r diffing, fig.height=3}
# Take the fourth lag to difference the beer data
beer_diff = diff(beer, lag = 4)

# Plot the resulting autocorrelation function
autoplot(acf(beer_diff, plot = FALSE))
```

If have autocorrelated data, and you aren't willing to difference the data or can't change the sampling rate or time step, you shouldn't use either run or control charts, and instead use a standard line chart. If you must have limits to help guide decision-making, you'll need a more advanced technique, such as a Generalized Additive Mixed Model (GAMM) or time series models such as ARIMA. It's probably best to work with a statistician if you need to do this.   

### What happens when you get the mean-variance relationship wrong

Although control charts can sometimes work when you misspecify the mean-variance relationship (they are "robust" to some assumption violations), you won't know unless you explore the differences in implications between the data as-is and that same data transformed to become more in line with the appropriate or expected distribution. 

For example, if you use the usual normal distribution control limits an *I* chart on exponentially-distributed data, you get something like this:

```{r skewy, fig.height=3.5}
# Create some fake exponentially-distributed process data
set.seed(290)
df2 = data.frame(x = seq(1:120), y = 17+rexp(120))

# Create plot object
exp_nat_var_cc_plot = ggplot(df2, aes(x, y)) + 
  ylim(14.75, 24.75) +
  geom_hline(aes(yintercept=17.88), color="gray", size=1) +
  geom_hline(aes(yintercept=20.87), color="red") +
  geom_hline(aes(yintercept=14.88), color="red") +
  geom_ribbon(aes(ymin = 18.87, ymax = 19.87), alpha = 0.2) +
  geom_ribbon(aes(ymin = 15.88, ymax = 16.88), alpha = 0.2) +
  xlab("Subgroup") + 
  ylab("Value") +
  geom_line() + 
  theme_bw()

# Marginal plot
ggMarginal(exp_nat_var_cc_plot, margins="y", type = "histogram", binwidth=0.25)
```

Clearly something is weird when no points even go below 1 standard deviation. But more importantly, do the points above the upper control limit represent *real* anomalous data points, or are they the result of an improper mean-variance relationship? 

Using a Box-Cox transformation to make the distribution more symmetrical, we can see that those seemingly out-of-control points are actually well within both control limits, and the variation we see is more in line with (statistical) expectation. 

```{r unskewy, fig.height=3.5}
# Box-Cox tansformation 
bob = data.frame(MASS::boxcox(df2$y ~ 1, lambda=seq(-20, 5, 0.5), plotit=F))
bobmax = bob[which.max(bob[,2]),1]

# Adjustment to make plotting cleaner
df2$y2 = (df2$y ^ bobmax) * 10^19

# Create plot object
exp_xform_nat_var_cc_plot = ggplot(df2, aes(x, y2)) + 
  ylim(-0.06, 0.31) +
  geom_hline(aes(yintercept=0.128), color="gray", size=1) +
  geom_hline(aes(yintercept=0.302), color="red") +
  geom_hline(aes(yintercept=-0.046), color="red") +
  geom_ribbon(aes(ymin = 0.186, ymax = 0.244), alpha = 0.2) +
  geom_ribbon(aes(ymin = 0.012, ymax = 0.070), alpha = 0.2) +
  xlab("Subgroup") + 
  ylab("Transformed Value") +
  geom_line() + 
  theme_bw()

# Marginal plot
ggMarginal(exp_xform_nat_var_cc_plot, margins="y", type = "histogram", binwidth=0.025)
```

The main drawback is that you now have a chart of essentially uninterptable values---but that's better than assuming a normal distribution will be just fine and being unnecessarily alarmed by false positive signals, wasting time and resources searching for a special cause that doesn't exist.    
