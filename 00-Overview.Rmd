# Signal, noise, and statisical process control {#SPC}

## Signal and noise

People are really good at finding patterns that aren't real.  

Every process has natural variation---*noise*---included as an inherent part of that process. True signals only emerge when you have properly characterized that variation. Statistical process control (SPC) charts---run charts and control charts---help you characterize and identify non-ranom patterns that suggest your process has changed. 

Statistical theory provides the basis by which we can evaluate processes to more confidently detect changes in process amongst the noise of natural variation.  

Understanding natural variation in time series or sequential data is the essential point of quality assurance or process improvement efforts. It's a rookie mistake to use SPC tools to focus solely on the values themselves or their central tendency. You need to look at [all the elements](#guidelines) of a run or control chart to understand what it's telling you.  

This graph shows a process created using random numbers based on a known normal distribution, where the overall distribution is shown in a histogram to the right of the run chart.  

<br>  

```{r ggmarg, fig.height=3, echo=FALSE}
set.seed(250)
df = data.frame(x = seq(1:120), y = 18+rnorm(120))

nat_var_run_plot = ggplot(df, aes(x, y)) + 
  ylim(14.75, 21.25) +
  geom_hline(aes(yintercept=18), color="gray", size=1) +
  xlab("Subgroup") + 
  ylab("Value") +
  geom_line() + 
  theme_bw()

ggMarginal(nat_var_run_plot, margins="y", type = "histogram", binwidth=0.5)
```

<br>  

The next plot shows control limits and 1-2$\sigma$ bands for comparison. Note that some of the control chart [guidelines](#guidelines) for detecting special causes suggest that some special cause variation has occurred in this data. Since this data was generated using random numbers from a known normal distribution, these are false positives. It's important to remember that control charts are meant to strike a balance true and false positives, but can never entirely eliminate the false signals.      

<br>  

```{r ggmarg_cc, fig.height=3, echo=FALSE}
nat_var_cc_plot = ggplot(df, aes(x, y)) + 
  ylim(14.75, 21.25) +
  geom_hline(aes(yintercept=18), color="gray", size=1) +
  geom_hline(aes(yintercept=20.96), color="red") +
  geom_hline(aes(yintercept=15.1), color="red") +
  geom_ribbon(aes(ymin = 18.98, ymax = 19.96), alpha = 0.2) +
  geom_ribbon(aes(ymin = 16.04, ymax = 17.02), alpha = 0.2) +
  xlab("Subgroup") + 
  ylab("Value") +
  geom_line() + 
  theme_bw()

ggMarginal(nat_var_cc_plot, margins="y", type = "histogram", binwidth=0.5)
```

<br>  

## SPC tools

Run and control charts are the basic tools of SPC analysis. Other basical statistical graphs---particularly line charts and histograms---are equally important to SPC work.    

Line charts help you monitor any sort of metric, process, or time series data. Run and control charts are meant to help you identify departures from a **stable** process. Each uses a set of guidelines to help you make decisions on whether a process has changed or not. 

In many cases, a run chart may be all you need. In *all* cases, you should [start with a line chart and histogram](#histoline). If---and only if---you need to characterize the limits of natural variation in a stable process, you can move on to using a control chart.  

In addition, *never* rely on a table or year-to-date (YTD) comparisons to evaluate process performance. These approaches ignore the foundational concept of process control: that natural, "common-cause" variation is an essential part of the process, and you can't see natural variation in a table or in YTD comparisons. Tables or YTD values can supplement run or control charts, but should never be used without them. 

Above all, remember that the decisions you make in constructing SPC charts *will* impact the interpretation of the results. Bad charts can make for bad decisions. 

## Stability

It takes time and resources to track new KPIs (collecting the data, developing the dashboards, etc), so you should be very clear on what the definition of "stability" is, and make sure these KPIs are actually useful in tracking that stability.

In many cases when folks talk about "stability" they mean "constant", and think of their stability KPI as trying to keep their previous KPI at some fixed value or achieve some fixed target. However, in the case of units which are getting many more FTEs and in the case of patient access that is facing increasing patient demand and constrained hospital capacity, you would not expect the situation---and thus the data representing the process---to be constant over time.

We expect the call center performance to improve (in terms of percent of calls answered in under 2 minutes) and access to suffer (in terms of percent of new visit seen within 2 weeks of calling for an appointment). So what does "stable” mean in a changing environment?

*Stability* means that the system is responding as we would expect to the changing environment and that the system is robust to adverse surprises from the environment. **KPIs should be specifically designed to track this.** Essentially any KPI should track whether the process or system is stable and robust, rather than focusing strictly on the outcome as defined by previous KPIs. We can explain with two examples.

*NOTE: MAKE THESE EXAMPLES GENERIC* 

In the past, the primary KPI for the call center has been percent of calls answered within 2 minutes. So, what would a "stable” call center look like as they add FTEs and the old KPI improves over time? Perhaps it would be that the performance of the various teams within the call center become more similar (e.g., decreased variability across teams). Maybe it could be the frequency of adverse events (e.g., people waiting longer than X minutes) staying below some threshold---similar to a "downtime" KPI used to track the stability of computer systems. Maybe it could be something along the lines of the percent change in the old KPI tracking the percent change in FTEs (though we know this relationship is non-linear). The point is, we know performance as previously defined will be improving in the call center, but we still want to track "stability" in the face of these improvements.

The current KPI for access is the percent of new visit patients who have to wait less than or equal to 14 days between the day they call to schedule their appointment and the date of their appointment. We expect this will be decreasing over the next year as demand is expected to rise at a faster rate than capacity for new visits. So, what would a stable system look like in this case? Right now, the thought is that the KPI could be something like the change in access that we would predict to occur based on actual changes in demand and capacity (prediction model developed from past data) compared to the actual change in access. In other words, is the system responding as it has in the past? The problem is that if actual changes deviate largely from predicted changes, we don’t know if that’s caused by an incorrect assumption in the prediction model or lack of stability in the actual system. 

In both of these cases, stability can not be defined simply as lack of change in the previous performance metrics. 

You should make sure the KPIs for stability are properly designed from the outset before you spend large amounts of resources to develop and track them. 

Another part will depend on the purpose of the KPI. If the purpose is to help you better understand, for example, patient access and to serve as a warning system for when you may need to dig deeper into both the model and the data, this (**WHAT IS THIS?**) should do fairly well. If the goal is to serve as a warning system to trigger corrective management actions, then you would want a level of accuracy in the predictions that could be difficult to achieve.  

